---
title: "Solutions to Project 1"
author: "Tom Jeon"
date: "September 10, 2016"
output:
    md_document:
        variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## NOTE :

If you're new to RStudio, you may need to download these packages to be able to knit this document:

```{r, eval=FALSE}
install.packages("rmarkdown")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("randomForest")
```

You also need to change the working directory in line 29 of this `.Rmd` file.

Knitting will take a little while especially at 96%, so just sit tight lol.

## Exploratory Analytics

```{r, warning=FALSE}
setwd("/Users/tomjeon/ugrid/Proj_1") # you need to change this to your own wd
dat <- read.csv("conversion_data.csv")
head(dat)
str(dat) # structure of data
```

First thing I do is to look for weird data points that suggests wrong data. I've already cleaned the data when I uploaded the project, but I put wrong data points in there on purpose.

```{r}
summary(dat)
```

So few things from this:

- the site is probably US, although Chinese presence is strong.
- user base is pretty young
- conversion rate is ~3%
- everything seems to make sense except max age is 123 years...

Let's investigate:

```{r, warning=FALSE}
sort(unique(dat$age), decreasing = TRUE)
subset(dat, age > 100)
```

Ok, so it's just two users. Not a big deal. We can just remove them and nothing would change. In general, depending on the problem, you can 

- remove the entire row, saying you just don't trust any data associated with that
- treat those values (in this case `age`) as `NA`s
- if there is a pattern, try to figure out what went wrong (schema design, some fundamental flaw in data collection process)

When you have a lot of data as we do here, it's safest to just delete the row.

```{r, warning=FALSE}
clean_dat <- subset(dat, age < 80) # just to be extra conservative
```

Now, let’s quickly investigate the variables and how their distribution differs for the two classes. This will help us understand whether there is any information in our data in the first place and get a sense of the data.

**It's good practice to always get a sense of the data before building any models.**

I'll just pick a couple of variables as an example, but you should do it with all:

```{r, warning=FALSE}
library(dplyr)
dat_country <- clean_dat %>%
  group_by(country) %>%
  summarise(conversion_rate = mean(converted))
  
# Visualizing above information
library(ggplot2)
ggplot(dat_country, aes(x = country, y = conversion_rate)) +
  geom_bar(stat = "identity", aes(fill = country))
```

So it's apparent that the Chinese convert at a much lower rate than other countries.

```{r, warning=FALSE}
dat_pages <- clean_dat %>%
  group_by(total_pages_visited) %>%
  summarise(conversion_rate = mean(converted))

# Plot
qplot(total_pages_visited, conversion_rate, data = dat_pages, geom = "line",
      main = "Proxy for time spent on site vs. probability of conversion")
```

I just want to mention that on these projects, focus on your strengths and interests. If visualization is your main strength/interest, spend as much time as you wish on that and come up with something great. If you ahve other strengths and interests, spend more time on those. These projects are pretty open-ended by design. By seeing where you spend more time, we each can also understand your strengths/interests and where you'd fit best in the future.

## Predictive Analytics

The goal for this project was to predict conversion rate. The outcome is binary so this is a *classification* problem. You care about insights to give product and marketing teams some ideas. Here are some of the methodologies you can use for a classification problem:

- Logistic regression
- Decision trees
- RuleFit
- Random Forest

I am going to pick a random forest to predict conversion rate. I pick a random forest because it usually requires very little time to optimize the parameters and it is strong with outliers, irrelevant variables, continuous and discrete variables. I'll use its partial dependence plots and variable importance to get insights about how the model got information from the data. Also, I will build a simple tree to find the most obvious user segments and see if they agree with RF partial dependence plots.

First, `converted` should really be a factor as well as `new_user`. Let's change them:
```{r, warning=FALSE}
clean_dat$converted <- as.factor(clean_dat$converted)
clean_dat$new_user <- as.factor(clean_dat$new_user)
levels(clean_dat$country)[levels(clean_dat$country) == "Germany"] = "DE" # easier to plot shorter names

# Test/training sets
train_sample <- sample(nrow(clean_dat), size = nrow(clean_dat) * 0.66)
train_dat <- clean_dat[train_sample, ]
test_dat <- clean_dat[-train_sample, ]

# Random forest
library(randomForest)
rf <- randomForest(y = train_dat$converted, x = train_dat[, -ncol(train_dat)],
                   ytest = test_dat$converted, xtest = test_dat[, -ncol(train_dat)],
                   ntree = 20, mtry = 3, keep.forest = TRUE)
rf
```

So, OOB error and test error are pretty similar... This means that we're not overfitting. Error is pretty low at about 1.5%.

Let's check variable importance:
```{r, warning=FALSE}
varImpPlot(rf, type = 2)
```

Total pages visited is the most important by far. Unfortunately, it is probably the least "actionable". People might visit many pages because they already know what they want to buy and in order to buy things you have to click on more pages.

Let's build the random forest without that variable. Since classes are heavily unbalanced and we don't have that very powerful variable anymore, I changed the weight a bit, just to make sure I'll get something classified as 1.

```{r, warning=FALSE}
rf2 <- randomForest(y = train_dat$converted, x = train_dat[, -c(5, ncol(train_dat))],
                   ytest = test_dat$converted, xtest = test_dat[, -c(5, ncol(train_dat))],
                   ntree = 20, mtry = 3, classwt = c(0.7, 0.3), keep.forest = TRUE)
rf2
```

Accuracy went down as expected but whatever. Model is still good to get some insights.

Rechecking variable importance:

```{r}
varImpPlot(rf2, type = 2)
```

Interesting. `new_user` is most important one now. Source doesn't seem to matter at all.

Now partial dependence plots for the 4 variables:
```{r, warning=FALSE}
library(rpart)
op <- par(mfrow = c(2, 2))
partialPlot(rf2, train_dat, country, 1)
partialPlot(rf2, train_dat, age, 1)
partialPlot(rf2, train_dat, new_user, 1)
partialPlot(rf2, train_dat, source, 1)
```

So this shows that:

- Users with an old account are much better than new users
- China is really bad
- The site works well for young people and bad for less young people (> 30 yrs)
- Source is irrelevant

Some conclusions and suggestions:

- The site is working very well for young users. Definitely let’s tell marketing to advertise and use marketing channel which are more likely to reach young people.
- The site is working very well for Germany in terms of conversion. But the summary showed that there are few Germans coming to the site: way less than UK, despite a larger population. Again, marketing should get more Germans. Big opportunity.
- Users with old accounts do much better. Targeted emails with offers to bring them back to the site could be a good idea to try.
- Something is wrong with the Chinese version of the site. It is either poorly translated, doesn’t fit the local culture, some payment issue or maybe it is just in English! Given how many users are based in China, fixing this should be a top priority. Huge opportunity.
- Maybe go through the UI and figure out why older users perform so poorly? From 30 y/o conversion clearly starts dropping.
- If I know someone has visited many pages, but hasn’t converted, she almost surely has high purchase intent. I could email her targeted offers or sending her reminders. Overall, these are probably the easiest users to make convert.

As you can see, conclusions usually end up being about:

- Tell marketing to get more of the good performing user segments.
- Tell product to fix the experience for the bad performing ones.